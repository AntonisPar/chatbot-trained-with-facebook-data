{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "charming-herald",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import json\n",
    "import csv\n",
    "import unicodedata\n",
    "import re\n",
    "from string import printable\n",
    "import pandas as pd\n",
    "import re\n",
    "from functools import partial\n",
    "from greeklish.converter import Converter\n",
    "#test1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ready-cowboy",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_emoji(df):\n",
    "  return df.applymap(lambda y: ''.join(filter(lambda x: x in printable, y)))\n",
    "\n",
    "myconverter = Converter(max_expansions=1)\n",
    "final_df = pd.DataFrame()\n",
    "\n",
    "fix_mojibake_escapes = partial(\n",
    "     re.compile(rb'\\\\u00([\\da-f]{2})').sub,\n",
    "     lambda m: bytes.fromhex(m.group(1).decode()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "engaged-maker",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "dir = \"inbox\"\n",
    "folders = os.listdir(dir)\n",
    "path_list = []\n",
    "paths_senders = {}\n",
    "for folder in folders:\n",
    "    for file in os.listdir(os.path.join(\"inbox\",folder)):\n",
    "        if file.startswith(\"message\"):\n",
    "            path = os.path.join(dir,folder,file)\n",
    "            try:\n",
    "                with open(path, 'rb') as file:\n",
    "                    repaired = fix_mojibake_escapes(file.read())\n",
    "                    data = json.loads(repaired.decode('utf8'))\n",
    "\n",
    "                    sender_name = data[\"participants\"][0][\"name\"]\n",
    "                    messages = data[\"messages\"]\n",
    "                    new_conversation = []\n",
    "                    for k in messages:\n",
    "                        if \"content\" in k.keys():\n",
    "                            new_conversation.append(k)\n",
    "                    pairs = []\n",
    "                    message = [] \n",
    "\n",
    "\n",
    "                    for m in new_conversation[::-1]:\n",
    "                        if len(message) == 0 or len(message) == 2:\n",
    "                            if m[\"sender_name\"] != \"Antonis Parlapanis\":\n",
    "                                message = []\n",
    "                                message.append(m[\"content\"])\n",
    "                        elif len(message) == 1:\n",
    "                            if m[\"sender_name\"] != \"Antonis Parlapanis\":\n",
    "                                message[0] = message[0]+ \" \" + m[\"content\"] \n",
    "                            else:\n",
    "                                message.append(m[\"content\"])\n",
    "                        else:\n",
    "                            if m[\"sender_name\"] == \"Antonis Parlapanis\":\n",
    "                                message[1] = message[1] + \" \" + m[\"content\"]\n",
    "                        pairs.append(message)\n",
    "                    new_pair = []\n",
    "                    for pair in pairs:\n",
    "                        if len(pair) == 2:\n",
    "                            new_pair.append(pair)\n",
    "\n",
    "\n",
    "                    me = []\n",
    "                    sender = []\n",
    "\n",
    "                    for pair in new_pair:\n",
    "                        sender.append(pair[0])\n",
    "                        me.append(pair[1])\n",
    "\n",
    "                    conv_df = pd.DataFrame()\n",
    "                    conv_df[\"sender\"] = np.array(sender)\n",
    "                    conv_df[\"me\"] = np.array(me)\n",
    "                    conv_df = conv_df.drop_duplicates(keep=\"first\")\n",
    "\n",
    "                    \n",
    "                    final_df = pd.concat([conv_df,final_df],axis=0)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "            except json.decoder.JSONDecodeError:\n",
    "                print(\"There was a problem accessing the equipment data.\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "final_df.drop(final_df[final_df[\"sender\"]==\"  \"].index,inplace=True)\n",
    "final_df.drop(final_df[final_df[\"me\"]==\"  \"].index,inplace=True)\n",
    "\n",
    "\n",
    "#\n",
    "final_df[\"sender\"] = final_df[\"sender\"].apply(lambda x: myconverter.convert(x))\n",
    "final_df[\"me\"] = final_df[\"me\"].apply(lambda x: myconverter.convert(x))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def normalize(s):\n",
    "    s = s.lower().strip()\n",
    "    s = re.sub(r\"([.!?])\", r\" \\1\", s)\n",
    "    s = re.sub(r\"[^a-zA-Z.!?]+\", r\" \", s)\n",
    "    return s\n",
    "\n",
    "final_df[\"sender\"] = final_df[\"sender\"].apply(lambda x: normalize(x[0]))\n",
    "final_df[\"me\"] = final_df[\"me\"].apply(lambda x: normalize(x[0]))\n",
    "\n",
    "print(final_df.head(20))\n",
    "\n",
    "\n",
    "final_df.to_csv('out.csv', sep='\\t', encoding='utf-8')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "timely-appendix",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "tokenizer=Tokenizer()\n",
    "tokenizer.fit_on_texts(final_df['sender'])\n",
    "train = tokenizer.texts_to_sequences(final_df['sender'])\n",
    "\n",
    "x_train = pad_sequences(train)\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "le = LabelEncoder()\n",
    "y_train = le.fit_transform(final_df['me'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "254e0423",
   "metadata": {},
   "outputs": [],
   "source": [
    "#input length\n",
    "input_shape = x_train.shape[1]\n",
    "print(input_shape)\n",
    "#define vocabulary\n",
    "vocabulary = len(tokenizer.word_index)\n",
    "print(\"number of unique words : \",vocabulary)\n",
    "#output length\n",
    "output_length = le.classes_.shape[0]\n",
    "print(\"output length: \",output_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f97893b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "import nltk\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.layers import Input, Embedding, LSTM , Dense,GlobalMaxPooling1D,Flatten\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "#creating the model\n",
    "i = Input(shape=(input_shape,))\n",
    "x = Embedding(vocabulary+1,10)(i)\n",
    "x = LSTM(10,return_sequences=True)(x)\n",
    "x = Flatten()(x)\n",
    "x = Dense(output_length,activation=\"softmax\")(x)\n",
    "model  = Model(i,x)\n",
    "#compiling the model\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\",optimizer='adam',metrics=['accuracy'])\n",
    "#training the model\n",
    "train = model.fit(x_train,y_train,epochs=20)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1f92a16",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e5c99bf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83b8afec",
   "metadata": {},
   "outputs": [],
   "source": [
    "#chatting\n",
    "import random\n",
    "import string\n",
    "while True:\n",
    "  texts_p = []\n",
    "  prediction_input = input('steile ston antwnh : ')\n",
    "  #removing punctuation and converting to lowercase\n",
    "  prediction_input = [letters.lower() for letters in prediction_input if letters not in string.punctuation]\n",
    "  prediction_input = ''.join(prediction_input)\n",
    "  texts_p.append(prediction_input)\n",
    "  #tokenizing and padding\n",
    "  prediction_input = tokenizer.texts_to_sequences(texts_p)\n",
    "  prediction_input = np.array(prediction_input).reshape(-1)\n",
    "  prediction_input = pad_sequences([prediction_input],input_shape)\n",
    "  #getting output from model\n",
    "  output = model.predict(prediction_input)\n",
    "  output = output.argmax()\n",
    "  #finding the right tag and predicting\n",
    "  response_tag = le.inverse_transform([output])[0]\n",
    "  print(response_tag)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
