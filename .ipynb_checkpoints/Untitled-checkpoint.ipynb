{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "charming-herald",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import json\n",
    "import csv\n",
    "import unicodedata\n",
    "import re\n",
    "from string import printable\n",
    "import pandas as pd\n",
    "import re\n",
    "from functools import partial\n",
    "#test1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ready-cowboy",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'partial' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-561381f44848>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mfinal_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m fix_mojibake_escapes = partial(\n\u001b[0m\u001b[1;32m      7\u001b[0m      \u001b[0mre\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mrb'\\\\u00([\\da-f]{2})'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msub\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m      lambda m: bytes.fromhex(m.group(1).decode()))\n",
      "\u001b[0;31mNameError\u001b[0m: name 'partial' is not defined"
     ]
    }
   ],
   "source": [
    "def remove_emoji(df):\n",
    "  return df.applymap(lambda y: ''.join(filter(lambda x: x in printable, y)))\n",
    "\n",
    "final_df = pd.DataFrame()\n",
    "\n",
    "fix_mojibake_escapes = partial(\n",
    "     re.compile(rb'\\\\u00([\\da-f]{2})').sub,\n",
    "     lambda m: bytes.fromhex(m.group(1).decode()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "engaged-maker",
   "metadata": {},
   "outputs": [],
   "source": [
    "dir = \"inbox\"\n",
    "folders = os.listdir(dir)\n",
    "path_list = []\n",
    "paths_senders = {}\n",
    "for folder in folders:\n",
    "    for file in os.listdir(os.path.join(\"inbox\",folder)):\n",
    "        if file.startswith(\"message\"):\n",
    "            path = os.path.join(dir,folder,file)\n",
    "            try:\n",
    "                with open(path, 'rb') as file:\n",
    "                    repaired = fix_mojibake_escapes(file.read())\n",
    "                    data = json.loads(repaired.decode('utf8'))\n",
    "\n",
    "                    sender_name = data[\"participants\"][0][\"name\"]\n",
    "                    messages = data[\"messages\"]\n",
    "                    new_conversation = []\n",
    "                    for k in messages:\n",
    "                        if \"content\" in k.keys():\n",
    "                            new_conversation.append(k)\n",
    "                    pairs = []\n",
    "                    message = [] #message pair\n",
    "\n",
    "\n",
    "                    for m in new_conversation[::-1]:\n",
    "                        if len(message) == 0 or len(message) == 2:\n",
    "                            if m[\"sender_name\"] != \"Antonis Parlapanis\":\n",
    "                                message = []\n",
    "                                message.append(m[\"content\"])\n",
    "                        elif len(message) == 1:\n",
    "                            if m[\"sender_name\"] != \"Antonis Parlapanis\":\n",
    "                                message[0] = message[0]+ \" \" + m[\"content\"] #enwnei duo munhmata sth seira\n",
    "                            else:\n",
    "                                message.append(m[\"content\"])\n",
    "                        else:\n",
    "                            if m[\"sender_name\"] == \"Antonis Parlapanis\":\n",
    "                                message[1] = message[1] + \" \" + m[\"content\"]\n",
    "                        pairs.append(message)\n",
    "                    new_pair = []\n",
    "                    for pair in pairs:\n",
    "                        if len(pair) == 2:\n",
    "                            new_pair.append(pair)\n",
    "\n",
    "\n",
    "                    me = []\n",
    "                    sender = []\n",
    "\n",
    "                    for pair in new_pair:\n",
    "                        sender.append(pair[0])\n",
    "                        me.append(pair[1])\n",
    "\n",
    "                    conv_df = pd.DataFrame()\n",
    "                    conv_df[\"sender\"] = np.array(sender)\n",
    "                    conv_df[\"me\"] = np.array(me)\n",
    "                    conv_df = conv_df.drop_duplicates(keep=\"first\")\n",
    "\n",
    "                    # conv_df = remove_emoji(conv_df)\n",
    "                    final_df = pd.concat([conv_df,final_df],axis=0)\n",
    "                    print(final_df)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "            except json.decoder.JSONDecodeError:\n",
    "                print(\"There was a problem accessing the equipment data.\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "final_df.drop(final_df[final_df[\"sender\"]==\"  \"].index,inplace=True)\n",
    "final_df.drop(final_df[final_df[\"me\"]==\"  \"].index,inplace=True)\n",
    "\n",
    "\n",
    "# Lowercase, trim, and remove non-letter characters\n",
    "def normalize(s):\n",
    "    s = s.lower().strip()\n",
    "    s = re.sub(r\"([.!?])\", r\" \\1\", s)\n",
    "    s = re.sub(r\"[^a-zA-Z.!?]+\", r\" \", s)\n",
    "    return s\n",
    "\n",
    "final_df[\"sender\"] = final_df[\"sender\"].apply(lambda x: normalize(x))\n",
    "final_df[\"me\"] = final_df[\"me\"].apply(lambda x: normalize(x))\n",
    "\n",
    "final_df.to_csv('out.csv', sep='\\t', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "timely-appendix",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1959 2202  516 ...  685  662  605]\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "\n",
    "tokenizer=Tokenizer()\n",
    "tokenizer.fit_on_texts(final_df['sender'])\n",
    "train = tokenizer.texts_to_sequences(final_df['sender'])\n",
    "\n",
    "x_train = pad_sequences(train)\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "le = LabelEncoder()\n",
    "y_train = le.fit_transform(final_df['me'])\n",
    "print(y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "254e0423",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "360\n",
      "number of unique words :  3628\n",
      "output length:  2462\n"
     ]
    }
   ],
   "source": [
    "#input length\n",
    "input_shape = x_train.shape[1]\n",
    "print(input_shape)\n",
    "#define vocabulary\n",
    "vocabulary = len(tokenizer.word_index)\n",
    "print(\"number of unique words : \",vocabulary)\n",
    "#output length\n",
    "output_length = le.classes_.shape[0]\n",
    "print(\"output length: \",output_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "f97893b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "432/432 [==============================] - 28s 63ms/step - loss: 6.2329 - accuracy: 0.2788\n",
      "Epoch 2/20\n",
      "432/432 [==============================] - 27s 62ms/step - loss: 5.7847 - accuracy: 0.2809\n",
      "Epoch 3/20\n",
      "432/432 [==============================] - 27s 62ms/step - loss: 5.5035 - accuracy: 0.2842\n",
      "Epoch 4/20\n",
      "432/432 [==============================] - 26s 61ms/step - loss: 5.1661 - accuracy: 0.3061\n",
      "Epoch 5/20\n",
      "432/432 [==============================] - 26s 61ms/step - loss: 4.8784 - accuracy: 0.3346\n",
      "Epoch 6/20\n",
      "432/432 [==============================] - 27s 62ms/step - loss: 4.6566 - accuracy: 0.3623\n",
      "Epoch 7/20\n",
      "432/432 [==============================] - 27s 63ms/step - loss: 4.4866 - accuracy: 0.3841\n",
      "Epoch 8/20\n",
      "432/432 [==============================] - 27s 63ms/step - loss: 4.3484 - accuracy: 0.4009\n",
      "Epoch 9/20\n",
      "432/432 [==============================] - 27s 63ms/step - loss: 4.2384 - accuracy: 0.4142\n",
      "Epoch 10/20\n",
      "432/432 [==============================] - 27s 63ms/step - loss: 4.1412 - accuracy: 0.4270\n",
      "Epoch 11/20\n",
      "432/432 [==============================] - 27s 64ms/step - loss: 4.0561 - accuracy: 0.4361\n",
      "Epoch 12/20\n",
      "432/432 [==============================] - 28s 64ms/step - loss: 3.9837 - accuracy: 0.4444\n",
      "Epoch 13/20\n",
      "432/432 [==============================] - 27s 64ms/step - loss: 3.9226 - accuracy: 0.4547\n",
      "Epoch 14/20\n",
      "432/432 [==============================] - 27s 63ms/step - loss: 3.8721 - accuracy: 0.4595\n",
      "Epoch 15/20\n",
      "432/432 [==============================] - 27s 64ms/step - loss: 3.8265 - accuracy: 0.4649\n",
      "Epoch 16/20\n",
      "432/432 [==============================] - 27s 63ms/step - loss: 3.7876 - accuracy: 0.4713\n",
      "Epoch 17/20\n",
      "432/432 [==============================] - 27s 64ms/step - loss: 3.7540 - accuracy: 0.4740\n",
      "Epoch 18/20\n",
      "432/432 [==============================] - 27s 64ms/step - loss: 3.7226 - accuracy: 0.4775\n",
      "Epoch 19/20\n",
      "432/432 [==============================] - 27s 64ms/step - loss: 3.6929 - accuracy: 0.4816\n",
      "Epoch 20/20\n",
      "432/432 [==============================] - 27s 63ms/step - loss: 3.6677 - accuracy: 0.4858\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "import nltk\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.layers import Input, Embedding, LSTM , Dense,GlobalMaxPooling1D,Flatten\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "#creating the model\n",
    "i = Input(shape=(input_shape,))\n",
    "x = Embedding(vocabulary+1,10)(i)\n",
    "x = LSTM(10,return_sequences=True)(x)\n",
    "x = Flatten()(x)\n",
    "x = Dense(output_length,activation=\"softmax\")(x)\n",
    "model  = Model(i,x)\n",
    "#compiling the model\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\",optimizer='adam',metrics=['accuracy'])\n",
    "#training the model\n",
    "train = model.fit(x_train,y_train,epochs=20)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1f92a16",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83b8afec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You : sto ellhnis?\n",
      "jason\n",
      "You : what\n",
      "mpainw\n",
      "You : pou\n",
      "\n",
      "You : re\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#chatting\n",
    "import random\n",
    "while True:\n",
    "  texts_p = []\n",
    "  prediction_input = input('You : ')\n",
    "  #removing punctuation and converting to lowercase\n",
    "  prediction_input = [letters.lower() for letters in prediction_input if letters not in string.punctuation]\n",
    "  prediction_input = ''.join(prediction_input)\n",
    "  texts_p.append(prediction_input)\n",
    "  #tokenizing and padding\n",
    "  prediction_input = tokenizer.texts_to_sequences(texts_p)\n",
    "  prediction_input = np.array(prediction_input).reshape(-1)\n",
    "  prediction_input = pad_sequences([prediction_input],input_shape)\n",
    "  #getting output from model\n",
    "  output = model.predict(prediction_input)\n",
    "  output = output.argmax()\n",
    "  #finding the right tag and predicting\n",
    "  response_tag = le.inverse_transform([output])[0]\n",
    "  print(response_tag)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
