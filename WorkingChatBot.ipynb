{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "charming-herald",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import json\n",
    "import csv\n",
    "import unicodedata\n",
    "import re\n",
    "from string import printable\n",
    "import pandas as pd\n",
    "import re\n",
    "from functools import partial\n",
    "from greeklish.converter import Converter\n",
    "#test1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ready-cowboy",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_emoji(df):\n",
    "  return df.applymap(lambda y: ''.join(filter(lambda x: x in printable, y)))\n",
    "\n",
    "myconverter = Converter(max_expansions=1)\n",
    "final_df = pd.DataFrame()\n",
    "\n",
    "fix_mojibake_escapes = partial(\n",
    "     re.compile(rb'\\\\u00([\\da-f]{2})').sub,\n",
    "     lambda m: bytes.fromhex(m.group(1).decode()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "engaged-maker",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There was a problem accessing the equipment data.\n",
      "                                               sender  \\\n",
      "0                       eimai poly thymwmenh mazi sou   \n",
      "3                                          pou eisai    \n",
      "5                                                geia   \n",
      "7                            kalaaaaa auto einai egw    \n",
      "11                       apo thn gefyra ths enorganhs   \n",
      "13                             ekana xronia pou eisia   \n",
      "16                               xaxaxaxaxa gkomenaki   \n",
      "24       re blhma danai set her own nickname to ss .    \n",
      "29                          ela twra etsi de fainetai   \n",
      "33          egw eixa gynaikologo alliws tha koimomoun   \n",
      "35                              kai ti tha kaneis smr   \n",
      "39                                               pote   \n",
      "41                                        okkk antwnh   \n",
      "44                                  forema h mplouza    \n",
      "47                                        panagia mou   \n",
      "49                  mias filhs mou gia na to balw egw   \n",
      "51                 nai e kala tha dw otan to dokimasw   \n",
      "54             aaaaan to balw tha anebasw k tha deis    \n",
      "56  nomizw oti oi mplouzes mou einai pio makries a...   \n",
      "59                                 xaxaxaxaxa pou pas   \n",
      "\n",
      "                                              me  \n",
      "0                                          giati  \n",
      "3                                          spiti  \n",
      "5                                    ti kaneis ?  \n",
      "7             e apo pou epneusthkes auto to move  \n",
      "11                              kaneis enorganh   \n",
      "13          ela xthes dn htan h katallhlh stigmh  \n",
      "16                                     e peripou  \n",
      "24                                       ahahhah  \n",
      "29                                      twra nai  \n",
      "33                      e nai auto einai to mood  \n",
      "35                              h amalia einai ?  \n",
      "39                      otan fugete ap to magazi  \n",
      "41                                           ela  \n",
      "44                                    forematara  \n",
      "47                                 poianhs einai  \n",
      "49                                  na to valeis  \n",
      "51                                   steile fwto  \n",
      "54                                    ahhaa kala  \n",
      "56  e ama einai toso konto forato mono sto spiti  \n",
      "59              pouthena apo xthes einai . . . .  \n"
     ]
    }
   ],
   "source": [
    "\n",
    "dir = \"inbox\"\n",
    "folders = os.listdir(dir)\n",
    "path_list = []\n",
    "paths_senders = {}\n",
    "for folder in folders:\n",
    "    for file in os.listdir(os.path.join(\"inbox\",folder)):\n",
    "        if file.startswith(\"message\"):\n",
    "            path = os.path.join(dir,folder,file)\n",
    "            try:\n",
    "                with open(path, 'rb') as file:\n",
    "                    repaired = fix_mojibake_escapes(file.read())\n",
    "                    data = json.loads(repaired.decode('utf8'))\n",
    "\n",
    "                    sender_name = data[\"participants\"][0][\"name\"]\n",
    "                    messages = data[\"messages\"]\n",
    "                    new_conversation = []\n",
    "                    for k in messages:\n",
    "                        if \"content\" in k.keys():\n",
    "                            new_conversation.append(k)\n",
    "                    pairs = []\n",
    "                    message = [] #message pair\n",
    "\n",
    "\n",
    "                    for m in new_conversation[::-1]:\n",
    "                        if len(message) == 0 or len(message) == 2:\n",
    "                            if m[\"sender_name\"] != \"Antonis Parlapanis\":\n",
    "                                message = []\n",
    "                                message.append(m[\"content\"])\n",
    "                        elif len(message) == 1:\n",
    "                            if m[\"sender_name\"] != \"Antonis Parlapanis\":\n",
    "                                message[0] = message[0]+ \" \" + m[\"content\"] #enwnei duo munhmata sth seira\n",
    "                            else:\n",
    "                                message.append(m[\"content\"])\n",
    "                        else:\n",
    "                            if m[\"sender_name\"] == \"Antonis Parlapanis\":\n",
    "                                message[1] = message[1] + \" \" + m[\"content\"]\n",
    "                        pairs.append(message)\n",
    "                    new_pair = []\n",
    "                    for pair in pairs:\n",
    "                        if len(pair) == 2:\n",
    "                            new_pair.append(pair)\n",
    "\n",
    "\n",
    "                    me = []\n",
    "                    sender = []\n",
    "\n",
    "                    for pair in new_pair:\n",
    "                        sender.append(pair[0])\n",
    "                        me.append(pair[1])\n",
    "\n",
    "                    conv_df = pd.DataFrame()\n",
    "                    conv_df[\"sender\"] = np.array(sender)\n",
    "                    conv_df[\"me\"] = np.array(me)\n",
    "                    conv_df = conv_df.drop_duplicates(keep=\"first\")\n",
    "\n",
    "                    # conv_df = remove_emoji(conv_df)\n",
    "                    final_df = pd.concat([conv_df,final_df],axis=0)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "            except json.decoder.JSONDecodeError:\n",
    "                print(\"There was a problem accessing the equipment data.\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "final_df.drop(final_df[final_df[\"sender\"]==\"  \"].index,inplace=True)\n",
    "final_df.drop(final_df[final_df[\"me\"]==\"  \"].index,inplace=True)\n",
    "\n",
    "\n",
    "#\n",
    "final_df[\"sender\"] = final_df[\"sender\"].apply(lambda x: myconverter.convert(x))\n",
    "final_df[\"me\"] = final_df[\"me\"].apply(lambda x: myconverter.convert(x))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Lowercase, trim, and remove non-letter characters\n",
    "def normalize(s):\n",
    "    s = s.lower().strip()\n",
    "    s = re.sub(r\"([.!?])\", r\" \\1\", s)\n",
    "    s = re.sub(r\"[^a-zA-Z.!?]+\", r\" \", s)\n",
    "    return s\n",
    "\n",
    "final_df[\"sender\"] = final_df[\"sender\"].apply(lambda x: normalize(x[0]))\n",
    "final_df[\"me\"] = final_df[\"me\"].apply(lambda x: normalize(x[0]))\n",
    "\n",
    "print(final_df.head(20))\n",
    "\n",
    "\n",
    "final_df.to_csv('out.csv', sep='\\t', encoding='utf-8')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "timely-appendix",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "tokenizer=Tokenizer()\n",
    "tokenizer.fit_on_texts(final_df['sender'])\n",
    "train = tokenizer.texts_to_sequences(final_df['sender'])\n",
    "\n",
    "x_train = pad_sequences(train)\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "le = LabelEncoder()\n",
    "y_train = le.fit_transform(final_df['me'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "254e0423",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "371\n",
      "number of unique words :  9029\n",
      "output length:  4775\n"
     ]
    }
   ],
   "source": [
    "#input length\n",
    "input_shape = x_train.shape[1]\n",
    "print(input_shape)\n",
    "#define vocabulary\n",
    "vocabulary = len(tokenizer.word_index)\n",
    "print(\"number of unique words : \",vocabulary)\n",
    "#output length\n",
    "output_length = le.classes_.shape[0]\n",
    "print(\"output length: \",output_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f97893b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "204/204 [==============================] - 19s 91ms/step - loss: 8.4483 - accuracy: 0.0312\n",
      "Epoch 2/20\n",
      "204/204 [==============================] - 19s 91ms/step - loss: 8.1426 - accuracy: 0.0415\n",
      "Epoch 3/20\n",
      "204/204 [==============================] - 19s 94ms/step - loss: 7.9748 - accuracy: 0.0435\n",
      "Epoch 4/20\n",
      "204/204 [==============================] - 19s 92ms/step - loss: 7.7113 - accuracy: 0.0435\n",
      "Epoch 5/20\n",
      "204/204 [==============================] - 19s 93ms/step - loss: 7.0510 - accuracy: 0.0567\n",
      "Epoch 6/20\n",
      "204/204 [==============================] - 19s 94ms/step - loss: 6.2774 - accuracy: 0.1082\n",
      "Epoch 7/20\n",
      "204/204 [==============================] - 20s 97ms/step - loss: 5.6139 - accuracy: 0.1798\n",
      "Epoch 8/20\n",
      "204/204 [==============================] - 19s 94ms/step - loss: 5.0764 - accuracy: 0.2414\n",
      "Epoch 9/20\n",
      "204/204 [==============================] - 19s 94ms/step - loss: 4.6357 - accuracy: 0.3007\n",
      "Epoch 10/20\n",
      "204/204 [==============================] - 19s 94ms/step - loss: 4.2648 - accuracy: 0.3498\n",
      "Epoch 11/20\n",
      "204/204 [==============================] - 19s 93ms/step - loss: 3.9505 - accuracy: 0.3898\n",
      "Epoch 12/20\n",
      "204/204 [==============================] - 19s 94ms/step - loss: 3.6775 - accuracy: 0.4299\n",
      "Epoch 13/20\n",
      "204/204 [==============================] - 19s 94ms/step - loss: 3.4385 - accuracy: 0.4657\n",
      "Epoch 14/20\n",
      "204/204 [==============================] - 19s 95ms/step - loss: 3.2359 - accuracy: 0.4921\n",
      "Epoch 15/20\n",
      "204/204 [==============================] - 19s 95ms/step - loss: 3.0520 - accuracy: 0.5168\n",
      "Epoch 16/20\n",
      "204/204 [==============================] - 20s 97ms/step - loss: 2.8883 - accuracy: 0.5445\n",
      "Epoch 17/20\n",
      "204/204 [==============================] - 20s 96ms/step - loss: 2.7400 - accuracy: 0.5646\n",
      "Epoch 18/20\n",
      "204/204 [==============================] - 20s 97ms/step - loss: 2.6080 - accuracy: 0.5847\n",
      "Epoch 19/20\n",
      "204/204 [==============================] - 20s 97ms/step - loss: 2.4770 - accuracy: 0.6045\n",
      "Epoch 20/20\n",
      "204/204 [==============================] - 20s 96ms/step - loss: 2.3649 - accuracy: 0.6214\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "import nltk\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.layers import Input, Embedding, LSTM , Dense,GlobalMaxPooling1D,Flatten\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "#creating the model\n",
    "i = Input(shape=(input_shape,))\n",
    "x = Embedding(vocabulary+1,10)(i)\n",
    "x = LSTM(10,return_sequences=True)(x)\n",
    "x = Flatten()(x)\n",
    "x = Dense(output_length,activation=\"softmax\")(x)\n",
    "model  = Model(i,x)\n",
    "#compiling the model\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\",optimizer='adam',metrics=['accuracy'])\n",
    "#training the model\n",
    "train = model.fit(x_train,y_train,epochs=20)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1f92a16",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e5c99bf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "83b8afec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "steile ston antwnh : geia\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'tokenizer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-9ab0f84c382b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m   \u001b[0mtexts_p\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprediction_input\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m   \u001b[0;31m#tokenizing and padding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m   \u001b[0mprediction_input\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtexts_to_sequences\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtexts_p\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m   \u001b[0mprediction_input\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprediction_input\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m   \u001b[0mprediction_input\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpad_sequences\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mprediction_input\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0minput_shape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'tokenizer' is not defined"
     ]
    }
   ],
   "source": [
    "#chatting\n",
    "import random\n",
    "import string\n",
    "while True:\n",
    "  texts_p = []\n",
    "  prediction_input = input('steile ston antwnh : ')\n",
    "  #removing punctuation and converting to lowercase\n",
    "  prediction_input = [letters.lower() for letters in prediction_input if letters not in string.punctuation]\n",
    "  prediction_input = ''.join(prediction_input)\n",
    "  texts_p.append(prediction_input)\n",
    "  #tokenizing and padding\n",
    "  prediction_input = tokenizer.texts_to_sequences(texts_p)\n",
    "  prediction_input = np.array(prediction_input).reshape(-1)\n",
    "  prediction_input = pad_sequences([prediction_input],input_shape)\n",
    "  #getting output from model\n",
    "  output = model.predict(prediction_input)\n",
    "  output = output.argmax()\n",
    "  #finding the right tag and predicting\n",
    "  response_tag = le.inverse_transform([output])[0]\n",
    "  print(response_tag)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
